{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "from cltk.tokenize.latin.sentence import SentenceTokenizer\n",
    "from cltk.tokenize.latin.word import WordTokenizer\n",
    "\n",
    "latin_sentence_tokenizer = SentenceTokenizer()\n",
    "latin_word_tokenizer = WordTokenizer()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\";\", \"\")\n",
    "    text = text.replace(\"?\", \"\")\n",
    "    text = text.replace(\"!\", \"\")\n",
    "    text = text.replace(\":\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace('\"', \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def extract_parsed_dlh_books(directory: str) -> List:\n",
    "    \"\"\"\n",
    "    TXT files to str\n",
    "\n",
    "    :param directory:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    retrieved_texts = []\n",
    "    book_filenames = os.listdir(directory)\n",
    "    book_filenames = sorted(book_filenames, key=lambda x: int(x.split(\".\")[0]))\n",
    "    for filename in book_filenames:\n",
    "        print(filename)\n",
    "        with codecs.open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            lines = [[latin_word_tokenizer.tokenize(remove_punctuation(sentence))\n",
    "                      for sentence in latin_sentence_tokenizer.tokenize(line.strip())]\n",
    "                     for line in text.split(\"\\n\") if line.strip()]\n",
    "        retrieved_texts.append(lines)\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cltk.corpus.latin.corpora import LATIN_CORPORA\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "#print(LATIN_CORPORA)\n",
    "ci = CorpusImporter(\"latin\")\n",
    "ci.import_corpus(\"latin_models_cltk\")\n",
    "ci.import_corpus(\"latin_text_perseus\")\n",
    "ci.import_corpus(\"latin_text_latin_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hunc', 'Unk'),\n",
       " ('ferunt', 'V3PPIA---'),\n",
       " ('instituisse', 'V--RNA---'),\n",
       " ('ecclesias', 'Unk'),\n",
       " ('per', 'R--------'),\n",
       " ('vicos', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('id', 'P-S---NN-'),\n",
       " ('est', 'V3SPIA---'),\n",
       " ('Calatonno', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('Bricca', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('Rotomago', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('Briotreide', 'Unk'),\n",
       " (',', 'U--------'),\n",
       " ('Cainone', 'Unk'),\n",
       " ('.', 'U--------')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.tag.pos import TAGGERS\n",
    "from cltk.tag.pos import POSTag\n",
    "latin_pos_tagger = POSTag(\"latin\")\n",
    "example = \"Hunc ferunt instituisse ecclesias per vicos, id est Calatonno, Bricca, Rotomago, Briotreide, Cainone.\"\n",
    "latin_pos_tagger.tag_tnt(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "2.txt\n",
      "3.txt\n",
      "4.txt\n",
      "5.txt\n",
      "6.txt\n",
      "7.txt\n",
      "8.txt\n",
      "9.txt\n",
      "10.txt\n"
     ]
    }
   ],
   "source": [
    "books = extract_parsed_dlh_books(\"gregory_of_tours_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "books[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books 10\n",
      "Number of paragraphs 1022\n",
      "Number of tokens 123422\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of books {len(books)}\")\n",
    "print(f\"Number of paragraphs {sum([len([paragraph for paragraph in book]) for book in books])}\")\n",
    "# print(f\"Number of sentences {len([sentence for book in books for paragraph in book for sentence in paragraph])}\")\n",
    "print(f\"Number of tokens {len([token for book in books for paragraph in book for sentence in paragraph for token in sentence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_set = set([token for book in [books[4]] for paragraph in book for sentence in paragraph for token in sentence[1:]])\n",
    "proper_nouns = set([word for word in tokens_set if word and word[0].isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['Sigyberthi', 'Sigiberthus', 'Sigibertho', 'Sigybertho', 'Siggonis', 'Sigyberthus', 'Sigiberthi', 'Sigivaldi', 'Sigymundum', 'Sigiberti']\n"
     ]
    }
   ],
   "source": [
    "print(\"Briotreide\" in proper_nouns)\n",
    "print([word for word in proper_nouns if word.startswith(\"Sig\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "\n",
    "example = \"\"\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "lemmata = lemmatizer.lemmatize(example)\n",
    "print(lemmata)\n",
    "\n",
    "lemmata_orig = lemmatizer.lemmatize(example, return_raw=True)\n",
    "print(lemmata_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatized_books = [lemmatizer.lemmatize(\" \".join(sentence), return_raw=True) for book in books for paragraph in book for sentence in paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pos_tagged_books = [\" \".join([\" \".join(sentence) for paragraph in book for sentence in paragraph]) for book in books]\n",
    "# pos_tagged_books[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pos_tagged_books = [latin_pos_tagger.tag_unigram(\" \".join([\" \".join(sentence) for paragraph in book for sentence in paragraph])) for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%timeit latin_pos_tagger.tag_unigram(\" \".join(books[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [sentence for book in books for paragraph in book for sentence in paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ecc6390f5dfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpos_tagged_books\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlatin_pos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_bigram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-ecc6390f5dfe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpos_tagged_books\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlatin_pos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_bigram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\clement_besnier\\pycharmprojects\\chr2020\\venv\\lib\\site-packages\\cltk\\tag\\pos.py\u001b[0m in \u001b[0;36mtag_bigram\u001b[1;34m(self, untagged_string)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0muntagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muntagged_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mpickle_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavailable_taggers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bigram'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mtagged_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muntagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtagged_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\clement_besnier\\pycharmprojects\\chr2020\\venv\\lib\\site-packages\\cltk\\utils\\file_operations.py\u001b[0m in \u001b[0;36mopen_pickle\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pos_tagged_books = [latin_pos_tagger.tag_bigram(\" \".join(sentence)) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_sentences = [lemmatizer.lemmatize(\" \".join(sentence), return_raw=True) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(sentences), len(pos_tagged_books), len(lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos_tagged_books[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that have unknown POS and same lemma as itself are likely with a capitalized first character are likely proper nouns.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_proper_nouns = []\n",
    "for i in range(len(sentences)):\n",
    "    # print(sentences[i], pos_tagged_books[i], lemmatized_sentences[i])\n",
    "    for j in range(len(sentences[i])):\n",
    "        proper_noun, pos_tag, res_lemma = sentences[i][j], pos_tagged_books[i][j], lemmatized_sentences[i][j]\n",
    "        if len(proper_noun) > 0 and proper_noun[0].isupper():\n",
    "            # print(res_lemma)\n",
    "            if len(res_lemma.split(\"/\")) == 1:\n",
    "                lemma = res_lemma\n",
    "            elif len(res_lemma.split(\"/\")) == 2:\n",
    "                lemma = res_lemma.split(\"/\")[1]\n",
    "            else:\n",
    "                continue\n",
    "            print(proper_noun, lemma, pos_tag)\n",
    "            if proper_noun.lower() == lemma.lower() and pos_tag[1] is None:\n",
    "                real_proper_nouns.append(proper_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(zip(pos_tags, res_lemmata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"Childeberthi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cltk.corpus.readers import get_corpus_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reader = get_corpus_reader(language=\"latin\", corpus_name=\"latin_text_latin_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "docs = list(reader.docs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reader._fileids = [fileid for fileid in reader._fileids if not fileid.startswith(\"grego\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(list(reader.docs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_tokens = set(reader.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the proper nouns that only occur in DLH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for_real_proper_nouns = set([p for p in real_proper_nouns if p not in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(for_real_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[i for i in all_tokens if i.startswith(\"Martin\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "already_known_proper_nouns = set([p for p in real_proper_nouns if p in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(already_known_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted(list(already_known_proper_nouns))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorted(list(already_known_proper_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"Sigiberthus\" in for_real_proper_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep them in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"dlh_proper_nouns.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(for_real_proper_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to keep regroup different forms of a lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cltk.text_reuse.levenshtein import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l = Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mat = np.zeros((len(for_real_proper_nouns), \n",
    "                len(for_real_proper_nouns)))\n",
    "for_real_proper_nouns = list(for_real_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(for_real_proper_nouns)):\n",
    "    for j in range(i):\n",
    "        mat[i, j] = l.Levenshtein_Distance(for_real_proper_nouns[i], for_real_proper_nouns[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for_real_proper_nouns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(len(for_real_proper_nouns)):\n",
    "    for j in range(i):\n",
    "        if 0 < mat[i, j] < 3:\n",
    "            l.append((for_real_proper_nouns[i], for_real_proper_nouns[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmata which have several forms in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = set([i for i, j in l ])\n",
    "t.update(set([j for i, j in l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted(list(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nouns which appear only with onecform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set([i for i in for_real_proper_nouns if i not in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
